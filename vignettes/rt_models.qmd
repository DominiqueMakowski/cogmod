---
title: "RT-only Models"
vignette: >
  %\VignetteIndexEntry{RT-only Models}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{quarto::html}
knitr:
  opts_chunk:
    collapse: true
    comment: '#>'
format: 
  html:
    toc: true
editor_options: 
  chunk_output_type: console
---

```{r}
#| label: setup
#| message: false
#| warning: false

library(cogmod)
library(easystats)
library(ggplot2)
library(brms)
library(cmdstanr)

options(mc.cores = parallel::detectCores() - 2)
```


## The Data

For this chapter, we will be using the data from [Wagenmakers et al., (2008)](https://doi.org/10.1016/j.jml.2007.04.006) - Experiment 1 also reanalyzed by [Heathcote & Love (2012)](https://doi.org/10.3389/fpsyg.2012.00292), that contains responses and response times for several participants in two conditions (where instructions emphasized either **speed** or **accuracy**).
Using the same procedure as the authors, we excluded all trials with uninterpretable response time, i.e., responses that are too fast (<200 ms instead of <180 ms) or too slow (>2 sec instead of >3 sec).

```{r}
#| code-fold: false

set.seed(123)  # For reproducibility

df <- read.csv("https://raw.githubusercontent.com/DominiqueMakowski/CognitiveModels/main/data/wagenmakers2008.csv")
df <- df[df$RT > 0.2 & df$Participant %in% c(1, 2, 3), ]

# Show 10 first rows
head(df, 10)
```

<!-- In the previous chapter, we modeled the error rate (the probability of making an error) using a logistic model, and observed that it was higher in the `"Speed"` condition.  -->
<!-- But how about speed?  -->
We are going to first take interest in the response times (RT) of **Correct** answers only (as we can assume that errors are underpinned by a different *generative process*). 

```{r}
#| output: false

df <- df[df$Error == 0, ]
df <- df[df$Condition == "Accuracy", ]
```


```{r}
ggplot(df, aes(x = RT)) +
  geom_histogram(bins = 120, alpha = 0.8, position = "identity") +
  theme_minimal()
```

## Models

::: {.panel-tabset}

### Normal

A basic linear model.

```{r}
#| eval: false
#| code-fold: true

f <- bf(
  RT ~ 1
)

m_normal <- brm(f,
  data = df, 
  init = 0,
  chains = 4, iter = 500, backend = "cmdstanr"
)

m_normal <- brms::add_criterion(m_normal, "loo") 

saveRDS(m_normal, file = "models/m_normal.rds")
```



### ExGaussian

```{r}
#| eval: false
#| code-fold: true

f <- bf(
  RT ~ 1,
  sigma ~ 1,
  beta ~ 1,
  family = exgaussian()
)

m_exgauss <- brm(f,
  data = df, 
  family = exgaussian(), 
  init = 0,
  chains = 4, iter = 500, backend = "cmdstanr"
)

m_exgauss <- brms::add_criterion(m_exgauss, "loo") 

saveRDS(m_exgauss, file = "models/m_exgauss.rds")
```


:::


## Model Comparison

### Model Fit

We can compare these models together using the `loo` package, which shows
that CHOCO provides a significantly better fit than the other models.

```{r}
#| echo: false

path <- "https://raw.github.com/DominiqueMakowski/cogmod/main/vignettes/models/"

m_normal <- readRDS(url(paste0(path, "m_normal.rds")))
m_exgauss <- readRDS(url(paste0(path, "m_exgauss.rds")))
m_lognormal <- readRDS(url(paste0(path, "m_lognormal.rds")))
m_wald <- readRDS(url(paste0(path, "m_wald.rds")))
m_weibull <- readRDS(url(paste0(path, "m_weibull.rds")))
m_logweibull <- readRDS(url(paste0(path, "m_logweibull.rds")))
m_invweibull <- readRDS(url(paste0(path, "m_invweibull.rds")))
m_gamma <- readRDS(url(paste0(path, "m_gamma.rds")))
m_invgamma <- readRDS(url(paste0(path, "m_invgamma.rds")))
```


```{r}
#| code-fold: false

# loo::loo_compare(m_normal, m_exgauss, m_lognormal, m_wald,
#                  m_weibull, m_logweibull, m_invweibull,
#                  m_gamma, m_invgamma) |>
#   parameters(include_ENP = TRUE)
```

### Posterior Predictive Check

`iterations` controls the actual number of iterations used (e.g., for the point-estimate) and `keep_iterations` the number included.

```{r}
#| code-fold: true


pred <- rbind(
  estimate_prediction(m_normal, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "Normal"),
  estimate_prediction(m_exgauss, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "ExGaussian"),
  estimate_prediction(m_lognormal, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "LogNormal"),
  estimate_prediction(m_wald, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "InvGaussian"),
  estimate_prediction(m_weibull, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "Weibull"),
  estimate_prediction(m_logweibull, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "LogWeibull"),
  estimate_prediction(m_invweibull, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "InvWeibull"),
  estimate_prediction(m_gamma, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "Gamma"),
  estimate_prediction(m_invgamma, keep_iterations = 100, iterations = 100) |>
    reshape_iterations() |>
    data_modify(Model = "InvGamma")
)


pred |> 
  ggplot(aes(x=iter_value)) +
  geom_histogram(data = df, aes(x=RT, y = after_stat(density)), 
                 fill = "black", bins=120) +
  geom_line(aes(color=Model, group=iter_group), stat="density", alpha=0.3) +
  theme_minimal() +
  facet_wrap(~Model) +
  coord_cartesian(xlim = c(0, 2)) +
  see::scale_color_material_d(guide = "none") +
  labs(y = "RT (s)")
```

